{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:30:45.668207Z",
     "iopub.status.busy": "2024-12-25T22:30:45.667644Z",
     "iopub.status.idle": "2024-12-25T22:30:57.196702Z",
     "shell.execute_reply": "2024-12-25T22:30:57.196002Z",
     "shell.execute_reply.started": "2024-12-25T22:30:45.668155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,  BatchNormalization, GlobalAveragePooling2D,  Input\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:30:57.198604Z",
     "iopub.status.busy": "2024-12-25T22:30:57.198165Z",
     "iopub.status.idle": "2024-12-25T22:31:00.228765Z",
     "shell.execute_reply": "2024-12-25T22:31:00.227897Z",
     "shell.execute_reply.started": "2024-12-25T22:30:57.198577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp /kaggle/input/kagglejson/kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:31:00.230298Z",
     "iopub.status.busy": "2024-12-25T22:31:00.230002Z",
     "iopub.status.idle": "2024-12-25T22:31:09.858828Z",
     "shell.execute_reply": "2024-12-25T22:31:09.857964Z",
     "shell.execute_reply.started": "2024-12-25T22:31:00.230270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:31:09.862109Z",
     "iopub.status.busy": "2024-12-25T22:31:09.861620Z",
     "iopub.status.idle": "2024-12-25T22:31:20.974198Z",
     "shell.execute_reply": "2024-12-25T22:31:20.973102Z",
     "shell.execute_reply.started": "2024-12-25T22:31:09.862044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d vipoooool/new-plant-diseases-dataset -p /kaggle/working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-11T20:34:21.786413Z",
     "iopub.status.busy": "2024-12-11T20:34:21.785839Z",
     "iopub.status.idle": "2024-12-11T20:35:13.009025Z",
     "shell.execute_reply": "2024-12-11T20:35:13.007929Z",
     "shell.execute_reply.started": "2024-12-11T20:34:21.786352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip /kaggle/working/new-plant-diseases-dataset.zip -d /kaggle/working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:35:13.010748Z",
     "iopub.status.busy": "2024-12-11T20:35:13.010433Z",
     "iopub.status.idle": "2024-12-11T20:35:34.995714Z",
     "shell.execute_reply": "2024-12-11T20:35:34.994802Z",
     "shell.execute_reply.started": "2024-12-11T20:35:13.010721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_image_dimensions(folder_path):\n",
    "    \"\"\"\n",
    "    Checks if all images in a folder and its subfolders have the same dimensions.\n",
    "\n",
    "    :param folder_path: Path to the main folder\n",
    "    :return: True if all images have the same dimensions, otherwise False and the inconsistent dimensions\n",
    "    \"\"\"\n",
    "    dimensions = None\n",
    "    inconsistent_files = []\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(folder_path))  # Count all files\n",
    "\n",
    "    # Traverse the folder and its subfolders\n",
    "    with tqdm(total=total_files, desc=\"Processing images\", unit=\"file\") as pbar:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Open the image file\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img_dimensions = img.size  # Returns (width, height)\n",
    "                        if dimensions is None:\n",
    "                            # Set the reference dimensions\n",
    "                            dimensions = img_dimensions\n",
    "                        elif img_dimensions != dimensions:\n",
    "                            # Add to the list of inconsistent files if dimensions don't match\n",
    "                            inconsistent_files.append((file_path, img_dimensions))\n",
    "                except Exception as e:\n",
    "                    # Handle errors when processing non-image files or corrupted images\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    if inconsistent_files:\n",
    "        # Print all files with inconsistent dimensions\n",
    "        print(\"The following images have inconsistent dimensions:\")\n",
    "        for file_path, dim in inconsistent_files:\n",
    "            print(f\"{file_path} - Dimensions: {dim}\")\n",
    "        return False\n",
    "    else:\n",
    "        # Print success message if all dimensions are consistent\n",
    "        print(\"All images have the same dimensions:\", dimensions)\n",
    "        return True\n",
    "\n",
    "# Specify the main folder to check\n",
    "folder_path = \"/kaggle/working/\"\n",
    "check_image_dimensions(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:35:34.996970Z",
     "iopub.status.busy": "2024-12-11T20:35:34.996680Z",
     "iopub.status.idle": "2024-12-11T20:35:38.087006Z",
     "shell.execute_reply": "2024-12-11T20:35:38.086098Z",
     "shell.execute_reply.started": "2024-12-11T20:35:34.996944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuration of the image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,  # Normalization: scales pixel values between 0 and 1\n",
    "    rotation_range=20,  # Random rotation up to 20 degrees\n",
    "    width_shift_range=0.2,  # Random horizontal shift up to 20% of the width\n",
    "    height_shift_range=0.2,  # Random vertical shift up to 20% of the height\n",
    "    zoom_range=0.2,  # Random zoom up to 20%\n",
    "    horizontal_flip=True,  # Random horizontal flip\n",
    "    fill_mode='nearest'  # Filling missing pixels with the nearest value\n",
    ")\n",
    "\n",
    "# Generator for the validation or test set (without augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255.0)  # Only normalization\n",
    "\n",
    "# Load images from the file system\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"/kaggle/working/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\",  # Path to the training dataset\n",
    "    target_size=(256, 256),  # Resize images to 224x224\n",
    "    batch_size=32,  # Number of images per batch\n",
    "    class_mode='categorical' , # Type of labels: 'categorical' for multi-class classification\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = validation_datagen.flow_from_directory(\n",
    "    \"/kaggle/working/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Visualization of 5 images before and after processing\n",
    "def plot_images_before_after(generator, num_images=5):\n",
    "    # Get a batch of images from the generator\n",
    "    images, _ = next(generator)\n",
    "\n",
    "    # Number of images to display\n",
    "    num_images = min(num_images, len(images))\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 3))\n",
    "    fig.suptitle('Original vs Augmented Images', fontsize=16)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Show the original image\n",
    "        axes[i, 0].imshow(images[i])\n",
    "        axes[i, 0].set_title(\"Original Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        print(\"Before:\",images[i])\n",
    "\n",
    "        # Create a new augmented image (simulate data augmentation)\n",
    "        augmented_img = train_datagen.random_transform(images[i])\n",
    "        axes[i, 1].imshow(augmented_img)\n",
    "        axes[i, 1].set_title(\"Augmented Image\")\n",
    "        axes[i, 1].axis('off')\n",
    "        print(\"After:\",augmented_img)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch from the validation generator to visualize the original images\n",
    "valid_generator.reset()  # Ensure the generator starts from the beginning\n",
    "images, labels = next(valid_generator)\n",
    "\n",
    "# Denormalize images for correct visualization\n",
    "#images = images * 255.0\n",
    "\n",
    "# Display 5 images before and after processing\n",
    "plot_images_before_after(valid_generator, num_images=5)\n",
    "\n",
    "print(\"Dictionary containing classes:\")\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "print(\"List of classes with index linked to dataset\")\n",
    "print(train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:39.892970Z",
     "iopub.status.busy": "2024-12-11T20:36:39.891775Z",
     "iopub.status.idle": "2024-12-11T20:36:39.901403Z",
     "shell.execute_reply": "2024-12-11T20:36:39.900668Z",
     "shell.execute_reply.started": "2024-12-11T20:36:39.892935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,  BatchNormalization, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:40.410146Z",
     "iopub.status.busy": "2024-12-11T20:36:40.409760Z",
     "iopub.status.idle": "2024-12-11T20:36:40.417148Z",
     "shell.execute_reply": "2024-12-11T20:36:40.416224Z",
     "shell.execute_reply.started": "2024-12-11T20:36:40.410118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def base_cnn_model(input_shape=(256, 256, 3), num_classes=38):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Custom CNN model for plant disease detection.\n",
    "\n",
    "    :param input_shape: Tuple representing the shape of input images.\n",
    "\n",
    "    :param num_classes: Integer, number of output classes.\n",
    "\n",
    "    :return: Compiled Keras model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 1\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 2\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 3\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Fully Connected Layers\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:40.454178Z",
     "iopub.status.busy": "2024-12-11T20:36:40.453923Z",
     "iopub.status.idle": "2024-12-11T20:36:41.517376Z",
     "shell.execute_reply": "2024-12-11T20:36:41.516511Z",
     "shell.execute_reply.started": "2024-12-11T20:36:40.454154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "model = base_cnn_model(input_shape=(256, 256, 3), num_classes=38)\n",
    "\n",
    "\n",
    "\n",
    "# Display the model architecture\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:41.519133Z",
     "iopub.status.busy": "2024-12-11T20:36:41.518837Z",
     "iopub.status.idle": "2024-12-11T20:36:41.714418Z",
     "shell.execute_reply": "2024-12-11T20:36:41.713120Z",
     "shell.execute_reply.started": "2024-12-11T20:36:41.519108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_model_architecture():\n",
    "\n",
    "    layers = [\n",
    "\n",
    "        \"\\nInput (256x256x3)\",\n",
    "\n",
    "        \"Conv2D (32 filters)\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Conv2D (64 filters)\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Conv2D (128 filters)\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Flatten\",\n",
    "\n",
    "        \"Dense (256 units)\",\n",
    "\n",
    "        \"Dropout\",\n",
    "\n",
    "        \"Dense (38 classes)\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a simple bar plot\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        plt.text(0.5, len(layers) - i - 0.5, layer, fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "        plt.hlines(y=len(layers) - i - 1, xmin=0.2, xmax=0.8, color=\"black\")\n",
    "\n",
    "    \n",
    "\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.title(\"Model Architecture\\n\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_model_architecture()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements to the Model\n",
    "\n",
    "\n",
    "\n",
    "To improve the architecture:\n",
    "\n",
    "\n",
    "\n",
    "1. **Add More Convolutional Layers**  \n",
    "\n",
    "   Increase the model's depth to extract more complex features.\n",
    "\n",
    "\n",
    "\n",
    "2. **Use Batch Normalization**  \n",
    "\n",
    "   Normalize activations within layers to stabilize and speed up training.\n",
    "\n",
    "\n",
    "\n",
    "3. **Adjust Dropout and Fully Connected Layers**  \n",
    "\n",
    "   Add more dense layers with dropouts for better generalization.\n",
    "\n",
    "\n",
    "\n",
    "4. **Global Average Pooling**  \n",
    "\n",
    "   Replace the `Flatten()` layer with `GlobalAveragePooling2D` for more efficient feature aggregation.\n",
    "\n",
    "\n",
    "\n",
    "5. **Learning Rate Scheduler**  \n",
    "\n",
    "   Include learning rate adjustment for smoother training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:41.716767Z",
     "iopub.status.busy": "2024-12-11T20:36:41.716315Z",
     "iopub.status.idle": "2024-12-11T20:36:41.727564Z",
     "shell.execute_reply": "2024-12-11T20:36:41.726608Z",
     "shell.execute_reply.started": "2024-12-11T20:36:41.716717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def improved_cnn_model(input_shape=(256, 256, 3), num_classes=38):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Improved CNN model for plant disease detection.\n",
    "\n",
    "    :param input_shape: Tuple representing the shape of input images.\n",
    "\n",
    "    :param num_classes: Integer, number of output classes.\n",
    "\n",
    "    :return: Compiled Keras model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 1\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 2\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 3\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Block 4\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    # Global Average Pooling instead of Flatten\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "\n",
    "    # Fully Connected Layers\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "    # Output Layer\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:42.356679Z",
     "iopub.status.busy": "2024-12-11T20:36:42.356046Z",
     "iopub.status.idle": "2024-12-11T20:36:42.548302Z",
     "shell.execute_reply": "2024-12-11T20:36:42.547441Z",
     "shell.execute_reply.started": "2024-12-11T20:36:42.356648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the improved CNN model\n",
    "\n",
    "improved_model = improved_cnn_model(input_shape=(256, 256, 3), num_classes=38)\n",
    "\n",
    "\n",
    "\n",
    "# Display the improved model architecture\n",
    "\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:42.708502Z",
     "iopub.status.busy": "2024-12-11T20:36:42.707671Z",
     "iopub.status.idle": "2024-12-11T20:36:42.920580Z",
     "shell.execute_reply": "2024-12-11T20:36:42.919697Z",
     "shell.execute_reply.started": "2024-12-11T20:36:42.708470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_improved_model_architecture():\n",
    "\n",
    "    layers = [\n",
    "\n",
    "        \"\\nInput (256x256x3)\",\n",
    "\n",
    "        \"Conv2D (32 filters) + BatchNorm\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Conv2D (64 filters) + BatchNorm\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Conv2D (128 filters) + BatchNorm\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"Conv2D (256 filters) + BatchNorm\",\n",
    "\n",
    "        \"MaxPooling2D\",\n",
    "\n",
    "        \"GlobalAveragePooling2D\",\n",
    "\n",
    "        \"Dense (512 units) + Dropout\",\n",
    "\n",
    "        \"Dense (256 units) + Dropout\",\n",
    "\n",
    "        \"Dense (38 classes)\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        plt.text(0.5, len(layers) - i - 0.5, layer, fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "        plt.hlines(y=len(layers) - i - 1, xmin=0.2, xmax=0.8, color=\"black\")\n",
    "\n",
    "    \n",
    "\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.title(\"Improved Model Architecture\\n\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_improved_model_architecture()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mobilenet_model(input_shape=(256, 256, 3), num_classes=38):\n",
    "\n",
    "\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "  \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False) \n",
    "    x = GlobalAveragePooling2D()(x) \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x) \n",
    "    outputs = Dense(num_classes, activation='softmax')(x) \n",
    "\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mobileNetModel = mobilenet_model()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:46.627853Z",
     "iopub.status.busy": "2024-12-11T20:36:46.626959Z",
     "iopub.status.idle": "2024-12-11T20:36:51.725849Z",
     "shell.execute_reply": "2024-12-11T20:36:51.724882Z",
     "shell.execute_reply.started": "2024-12-11T20:36:46.627783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the schedule\n",
    "initial_learning_rate = 1e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,  # Number of steps before decay\n",
    "    decay_rate=0.96,    # Decay factor\n",
    "    staircase=True      # Apply decay in discrete intervals\n",
    ")\n",
    "\n",
    "# Compile the model with the schedule\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='categorical_crossentropy',\n",
    "               metrics=[\n",
    "                'accuracy',  # Default accuracy\n",
    "                tf.keras.metrics.Precision(name='precision'),  # Precision metric\n",
    "                tf.keras.metrics.Recall(name='recall'),        # Recall metric\n",
    "                tf.keras.metrics.AUC(name='auc'),             # AUC metric\n",
    "            ])\n",
    "\n",
    "improved_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='categorical_crossentropy',\n",
    "               metrics=[\n",
    "                'accuracy',  # Default accuracy\n",
    "                tf.keras.metrics.Precision(name='precision'),  # Precision metric\n",
    "                tf.keras.metrics.Recall(name='recall'),        # Recall metric\n",
    "                tf.keras.metrics.AUC(name='auc'),             # AUC metric\n",
    "            ])\n",
    "\n",
    "mobileNetModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='categorical_crossentropy',\n",
    "               metrics=[\n",
    "                'accuracy',  # Default accuracy\n",
    "                tf.keras.metrics.Precision(name='precision'),  # Precision metric\n",
    "                tf.keras.metrics.Recall(name='recall'),        # Recall metric\n",
    "                tf.keras.metrics.AUC(name='auc'),             # AUC metric\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:36:59.452057Z",
     "iopub.status.busy": "2024-12-11T20:36:59.451590Z",
     "iopub.status.idle": "2024-12-11T23:05:08.310778Z",
     "shell.execute_reply": "2024-12-11T23:05:08.310056Z",
     "shell.execute_reply.started": "2024-12-11T20:36:59.452023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=40,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history2 = improved_model.fit(\n",
    "    train_generator,  \n",
    "    validation_data=valid_generator,\n",
    "    epochs=40,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history3 = mobileNetModel.fit(\n",
    "    train_generator,  \n",
    "    validation_data=valid_generator,\n",
    "    epochs=40,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:06:41.800186Z",
     "iopub.status.busy": "2024-12-11T23:06:41.799806Z",
     "iopub.status.idle": "2024-12-11T23:06:42.961911Z",
     "shell.execute_reply": "2024-12-11T23:06:42.960945Z",
     "shell.execute_reply.started": "2024-12-11T23:06:41.800158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the final model after training\n",
    "model.save('model.h5')\n",
    "\n",
    "# Optionally, save the history for later analysis\n",
    "import pickle\n",
    "\n",
    "with open('history.pkl', 'wb') as file:\n",
    "    pickle.dump(history1.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:12:58.636245Z",
     "iopub.status.busy": "2024-12-11T23:12:58.635880Z",
     "iopub.status.idle": "2024-12-11T23:12:59.165306Z",
     "shell.execute_reply": "2024-12-11T23:12:59.164452Z",
     "shell.execute_reply.started": "2024-12-11T23:12:58.636214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the combined history\n",
    "def plot_training_history(history):\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_history(history1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_image_dimensions(folder_path):\n",
    "    \"\"\"\n",
    "    Checks if all images in a folder and its subfolders have the same dimensions.\n",
    "\n",
    "    :param folder_path: Path to the main folder\n",
    "    :return: True if all images have the same dimensions, otherwise False and the inconsistent dimensions\n",
    "    \"\"\"\n",
    "    dimensions = None\n",
    "    inconsistent_files = []\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(folder_path))  # Count all files\n",
    "\n",
    "    # Traverse the folder and its subfolders\n",
    "    with tqdm(total=total_files, desc=\"Processing images\", unit=\"file\") as pbar:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Open the image file\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img_dimensions = img.size  # Returns (width, height)\n",
    "                        if dimensions is None:\n",
    "                            # Set the reference dimensions\n",
    "                            dimensions = img_dimensions\n",
    "                        elif img_dimensions != dimensions:\n",
    "                            # Add to the list of inconsistent files if dimensions don't match\n",
    "                            idnconsistent_files.append((file_path, img_dimensions))\n",
    "                except Exception as e:\n",
    "                    # Handle errors when processing non-image files or corrupted images\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    if inconsistent_files:\n",
    "        # Print all files with inconsistent dimensions\n",
    "        print(\"The following images have inconsistent dimensions:\")\n",
    "        for file_path, dim in inconsistent_files:\n",
    "            print(f\"{file_path} - Dimensions: {dim}\")\n",
    "        return False\n",
    "    else:\n",
    "        # Print success message if all dimensions are consistent\n",
    "        print(\"All images have the same dimensions:\", dimensions)\n",
    "        return True\n",
    "\n",
    "# Specify the main folder to check\n",
    "folder_path = \"/kaggle/working/\"\n",
    "check_image_dimensions(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to load and evaluate a model\n",
    "def load_and_evaluate_model(model_path, test_generator):\n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Evaluando modelo: {model_path}\")\n",
    "\n",
    "    # Make predictions\n",
    "    test_generator.reset()\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_generator.classes  # Get the real labels\n",
    "    class_labels = list(test_generator.class_indices.keys())  # Class labels\n",
    "\n",
    "    return predicted_classes, true_classes, class_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setting up the generator for raw images\n",
    "test_generator = validation_datagen.flow_from_directory(\n",
    "    directory=\"/kaggle/working/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/valid\",\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate the Model 4\n",
    "predicted_classes4, true_classes4, class_labels4 = load_and_evaluate_model(\n",
    "    'baselinemodel.h5', test_generator\n",
    ")\n",
    "\n",
    "# # Evaluate the Model 5\n",
    "predicted_classes5, true_classes5, class_labels5 = load_and_evaluate_model(\n",
    "    'improvedmodel.h5', test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#CLASSIFICATION REPORT\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def print_classification_report(model_name, true_classes, predicted_classes, class_labels):\n",
    "    # Print the classification report with model name\n",
    "    print(f\"\\nClassification Report for {model_name}:\\n\")\n",
    "    report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# # Print the Classification Report for Model 1\n",
    "# print_classification_report(\"Model 1\", true_classes1, predicted_classes1, class_labels1)\n",
    "\n",
    "# Print the Classification Report for Model 2\n",
    "# print_classification_report(\"Model 2\", true_classes2, predicted_classes2, class_labels2)\n",
    "\n",
    "# # Print the Classification Report for Model 4\n",
    "print_classification_report(\"Model 4\", true_classes4, predicted_classes4, class_labels4)\n",
    "\n",
    "# # Print the Classification Report for Model 5\n",
    "print_classification_report(\"Model 5\", true_classes5, predicted_classes5, class_labels5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#CONFUSION MATRIX\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_and_metrics(true_classes, predicted_classes, class_labels, title=\"Confusion Matrix\"):\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes, labels=range(len(class_labels)))\n",
    "\n",
    "    # Adjust figure size to avoid clutter\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))  # Larger figure size for more space\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=True)\n",
    "\n",
    "    # Adjust label rotation and size\n",
    "    ax.set_xticklabels(class_labels, rotation=90, fontsize=10, ha='center')  # Vertical labels\n",
    "    ax.set_yticklabels(class_labels, fontsize=10)\n",
    "\n",
    "    # Adjust text size and spacing inside the matrix\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(8)       # Adjust font size\n",
    "        text.set_ha('center')     # Center align\n",
    "        text.set_va('center')     # Vertical align\n",
    "        text.set_position((text.get_position()[0], text.get_position()[1] - 0.2))  # Adjust spacing\n",
    "\n",
    "    # Add title to the plot\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for Model 1\n",
    "# plot_confusion_and_metrics(true_classes1, predicted_classes1, class_labels1, title=\"Confusion Matrix - Model 1\")\n",
    "\n",
    "# # Plot results for Model 2\n",
    "# plot_confusion_and_metrics(true_classes2, predicted_classes2, class_labels2, title=\"Confusion Matrix - Model 2\")\n",
    "\n",
    "# # Plot results for Model 4\n",
    "plot_confusion_and_metrics(true_classes4, predicted_classes4, class_labels4, title=\"Confusion Matrix - Model 4\")\n",
    "\n",
    "# Plot results for Model 5\n",
    "plot_confusion_and_metrics(true_classes5, predicted_classes5, class_labels5, title=\"Confusion Matrix - Model 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ROC and AUC Curves\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def get_predicted_probabilities(model_path, test_generator):\n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Reset the generator and get the predicted probabilities\n",
    "    test_generator.reset()\n",
    "    predicted_probabilities = model.predict(test_generator, verbose=1)\n",
    "    return predicted_probabilities\n",
    "\n",
    "# Obtaining the probabilities for Model 1\n",
    "# predicted_probabilities1 = get_predicted_probabilities('/content/final_model20+20_128_1e-5.h5', test_generator)\n",
    "\n",
    "# Obtaining the probabilities for Model 2\n",
    "# predicted_probabilities2 = get_predicted_probabilities('final_improved_model10+10,128.h5', test_generator)\n",
    "\n",
    "# Obtaining the probabilities for Model 4\n",
    "predicted_probabilities4 = get_predicted_probabilities('baselinemodel.h5', test_generator)\n",
    "\n",
    "# Obtaining the probabilities for Model 5\n",
    "predicted_probabilities5 = get_predicted_probabilities('improvedmodel.h5', test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc_curve(true_classes, predicted_probabilities, class_labels, model_name):\n",
    "    # Convert true classes to binary format (One-Hot Encoding)\n",
    "    true_classes_bin = label_binarize(true_classes, classes=range(len(class_labels)))\n",
    "\n",
    "    # Initialize the figure for the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot the ROC curve for each class\n",
    "    for i in range(len(class_labels)):\n",
    "        fpr, tpr, thresholds = roc_curve(true_classes_bin[:, i], predicted_probabilities[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Random ROC curve (diagonal line)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "    # Final adjustments to the chart\n",
    "    plt.title(f'ROC curves for {model_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Place the legend outside the graph to avoid overlaps\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, ncol=1)\n",
    "\n",
    "    # Adjust the layout so that the labels do not overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#  Plot the ROC and AUC Curves for Model 1\n",
    "# plot_roc_curve(true_classes1, predicted_probabilities1, class_labels1, model_name=\"Model 1\")\n",
    "\n",
    "# # Plot the ROC and AUC Curves for Model 2\n",
    "# plot_roc_curve(true_classes2, predicted_probabilities1, class_labels2, model_name=\"Model 2\")\n",
    "\n",
    "# Plot the ROC and AUC Curves for Model 4\n",
    "plot_roc_curve(true_classes4, predicted_probabilities4, class_labels4, model_name=\"Model 4\")\n",
    "\n",
    "# Plot the ROC and AUC Curves for Model 5\n",
    "plot_roc_curve(true_classes5, predicted_probabilities5, class_labels5, model_name=\"Model 5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_overall_auc(true_classes, predicted_probabilities, class_labels):\n",
    "    # Convert true classes to binary format (One-Hot Encoding)\n",
    "    true_classes_bin = label_binarize(true_classes, classes=range(len(class_labels)))\n",
    "\n",
    "    # Calculate AUC for each class\n",
    "    auc_per_class = []\n",
    "    for i in range(len(class_labels)):\n",
    "        roc_auc = roc_auc_score(true_classes_bin[:, i], predicted_probabilities[:, i])\n",
    "        auc_per_class.append(roc_auc)\n",
    "\n",
    "    # Calculate the average AUC\n",
    "    overall_auc = np.mean(auc_per_class)\n",
    "    print(f\"AUC per class: {auc_per_class}\")\n",
    "    print(f\"Overall AUC (Average): {overall_auc:.2f}\")\n",
    "    return overall_auc\n",
    "\n",
    "# Example for Model 1\n",
    "overall_auc1 = calculate_overall_auc(true_classes4, predicted_probabilities4, class_labels4)\n",
    "# overall_auc2 = calculate_overall_auc(true_classes5, predicted_probabilities5, class_labels5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet needs Tensoflow version 2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the Model 3\n",
    "predicted_classes3, true_classes3, class_labels3 = load_and_evaluate_model(\n",
    "    'mobileNet20+20,128,1e-5.h5', test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print the Classification Report for Model 3\n",
    "print_classification_report(\"Model 3\", true_classes3, predicted_classes3, class_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot results for Model 3\n",
    "plot_confusion_and_metrics(true_classes3, predicted_classes3, class_labels3, title=\"Confusion Matrix - Model 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Obtaining the probabilities for Model 3\n",
    "predicted_probabilities3 = get_predicted_probabilities('/content/mobileNet20+20,128,1e-5.h5', test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(true_classes3, predicted_probabilities3, class_labels3, model_name=\"Model 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the overall AUC\n",
    "def calculate_overall_auc(true_classes, predicted_probabilities, class_labels):\n",
    "    true_classes_bin = label_binarize(true_classes, classes=range(len(class_labels)))\n",
    "    auc_values = [auc(*roc_curve(true_classes_bin[:, i], predicted_probabilities[:, i])[:2]) for i in range(len(class_labels))]\n",
    "    overall_auc = np.mean(auc_values)\n",
    "    print(f\"Overall AUC: {overall_auc:.2f}\")\n",
    "    return overall_auc\n",
    "\n",
    "overall_auc3 = calculate_overall_auc(true_classes3, predicted_probabilities3, class_labels3)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6205666,
     "sourceId": 10068679,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6276410,
     "sourceId": 10163874,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6276498,
     "sourceId": 10163975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6283911,
     "sourceId": 10174028,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6284481,
     "sourceId": 10174786,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
